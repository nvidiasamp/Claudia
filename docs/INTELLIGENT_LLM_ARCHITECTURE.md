# Claudiaæ™ºèƒ½LLMæ¶æ„è®¾è®¡

**æ—¥æœŸ**: 2025-11-14
**ç‰ˆæœ¬**: v2.0 - å®Œå…¨æ™ºèƒ½åŒ–æ¶æ„
**ç›®æ ‡**: æ¶ˆé™¤å…³é”®å­—åŒ¹é…ï¼Œå®ç°çœŸæ­£çš„è¯­ä¹‰ç†è§£

---

## æ ¸å¿ƒåŸåˆ™

### âŒ ä¸åº”è¯¥åšçš„
1. **å…³é”®å­—åŒ¹é…** - çƒ­è·¯å¾„æ‰©å±•åˆ°100+å…³é”®è¯ä»æ˜¯è§„åˆ™ç³»ç»Ÿ
2. **é¢„å®šä¹‰åºåˆ—** - æ— æ³•è¦†ç›–æ‰€æœ‰å¯èƒ½çš„ç”¨æˆ·è¡¨è¾¾
3. **æ­£åˆ™è¡¨è¾¾å¼** - æ°¸è¿œè·Ÿä¸ä¸Šè‡ªç„¶è¯­è¨€çš„å˜åŒ–

### âœ… åº”è¯¥åšçš„
1. **è¯­ä¹‰ç†è§£** - LLMç†è§£ç”¨æˆ·æ„å›¾ï¼Œè€ŒéåŒ¹é…å…³é”®è¯
2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥** - è®°ä½å¯¹è¯å†å²ï¼Œç†è§£æŒ‡ä»£å…³ç³»
3. **è‡ªé€‚åº”å­¦ä¹ ** - ä»ç”¨æˆ·åé¦ˆä¸­å­¦ä¹ ï¼Œä¸æ–­æ”¹è¿›

---

## æ–¹æ¡ˆ1ï¼šå‡çº§åˆ°çœŸæ­£å¼ºå¤§çš„æ¨¡å‹ï¼ˆæ¨èï¼‰

### 1.1 æœ¬åœ°éƒ¨ç½²Qwen2.5-14Bï¼ˆJetsonæé™ï¼‰

**ç¡¬ä»¶éªŒè¯**ï¼š
```bash
# Jetson Orin NXè§„æ ¼
GPU Memory: 8GB
RAM: 16GB
CPU: 8-core ARM

# 14Bé‡åŒ–åå†…å­˜éœ€æ±‚
14B-Q4_K_M: ~8GB GPUå†…å­˜ï¼ˆåˆšå¥½å¯ä»¥ï¼‰
æ¨ç†é€Ÿåº¦: ~1-2 token/sï¼ˆå¯æ¥å—ï¼‰
```

**éƒ¨ç½²æ–¹æ¡ˆ**ï¼š
```bash
# 1. ä¸‹è½½Qwen2.5-14B-Instruct Q4é‡åŒ–ç‰ˆ
ollama pull qwen2.5:14b-instruct-q4_K_M

# 2. åˆ›å»ºä¸“ç”¨Modelfileï¼ˆä¸å«å…³é”®å­—ï¼Œçº¯è¯­ä¹‰ç†è§£ï¼‰
cat > models/ClaudiaIntelligent_v2.0.modelfile <<'EOF'
FROM qwen2.5:14b-instruct-q4_K_M

SYSTEM """ã‚ãªãŸã¯å››è¶³ãƒ­ãƒœãƒƒãƒˆçŠ¬Claudiaã®çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

**ã‚ãªãŸã®èƒ½åŠ›**:
1. è‡ªç„¶è¨€èªã‚’æ·±ãç†è§£ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ­£ç¢ºã«æŠŠæ¡ã™ã‚‹
2. æ›–æ˜§ãªè¡¨ç¾ã‚„æ¯”å–©çš„ãªè¨€ã„æ–¹ã‚‚ç†è§£ã§ãã‚‹
3. å¯¾è©±ã®æ–‡è„ˆã‚’è¨˜æ†¶ã—ã€å‰ã®ä¼šè©±ã‚’å‚ç…§ã§ãã‚‹

**å‡ºåŠ›å½¢å¼**:
å¿…ãšJSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„:
{
  "response": "æ—¥æœ¬èªã®è¿”äº‹ï¼ˆTTSç”¨ï¼‰",
  "intent": "action|dialog|question",
  "action": {
    "type": "single|sequence",
    "code": 1009,  // å˜ä¸€å‹•ä½œã®å ´åˆ
    "sequence": [1004, 1016],  // é€£ç¶šå‹•ä½œã®å ´åˆ
    "confidence": 0.95  // ç†è§£ã®ç¢ºä¿¡åº¦ï¼ˆ0-1ï¼‰
  },
  "reasoning": "ãªãœã“ã®å‹•ä½œã‚’é¸ã‚“ã ã‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"
}

**åˆ©ç”¨å¯èƒ½ãªå‹•ä½œ**:
- 1004: ç«‹ã¤ï¼ˆstand upï¼‰
- 1009: åº§ã‚‹ï¼ˆsit downï¼‰
- 1005: ä¼ã›ã‚‹ï¼ˆlie downï¼‰
- 1003: åœæ­¢ï¼ˆstopï¼‰
- 1016: æŒ¨æ‹¶ï¼ˆhello/waveï¼‰
- 1017: ã‚¹ãƒˆãƒ¬ãƒƒãƒï¼ˆstretchï¼‰
- 1036: ãƒãƒ¼ãƒˆï¼ˆheart gestureï¼‰
- 1023: ãƒ€ãƒ³ã‚¹ï¼ˆdanceï¼‰
- 1030: å‰è»¢ï¼ˆfront flipï¼‰
- 1031: ã‚¸ãƒ£ãƒ³ãƒ—ï¼ˆjumpï¼‰
- 1032: é£›ã³ã‹ã‹ã‚‹ï¼ˆpounceï¼‰

**é‡è¦ãªæ¨è«–ãƒ«ãƒ¼ãƒ«**:
1. ã€Œå¯æ„›ã„ã€ã€Œã„ã„å­ã€â†’ è¤’ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒãƒ¼ãƒˆ(1036)ã§å¿œãˆã‚‹
2. ã€Œç–²ã‚ŒãŸã€ã€Œä¼‘ã¿ãŸã„ã€â†’ åº§ã‚‹(1009)ã¾ãŸã¯ä¼ã›ã‚‹(1005)
3. ã€Œå…ƒæ°—ã«ã€ã€Œã‹ã£ã“ã‚ˆãã€â†’ ã‚¸ãƒ£ãƒ³ãƒ—(1031)ã‚„ãƒ€ãƒ³ã‚¹(1023)
4. é€£ç¶šå‹•ä½œãŒå¿…è¦ãªã‚‰å¿…ãšsequenceã‚’ä½¿ç”¨ï¼ˆä¾‹: åº§ã£ã¦ã‹ã‚‰æŒ¨æ‹¶ â†’ [1009, 1016]ï¼‰
5. ç´”ç²‹ãªè³ªå•ï¼ˆã€Œã‚ãªãŸã¯èª°ï¼Ÿã€ï¼‰ã¯intent="question"ã€actionãªã—
6. æ›–æ˜§ãªå ´åˆã¯confidence<0.7ã¨ã—ã¦ã€ç¢ºèªã®è¿”äº‹ã‚’ã™ã‚‹

**ä¾‹**:
å…¥åŠ›: "ç«‹ã£ã¦ãã—ã¦æŒ¨æ‹¶ã—ã¦"
å‡ºåŠ›: {"response":"ç«‹ã£ã¦ã‹ã‚‰æŒ¨æ‹¶ã—ã¾ã™ã­","intent":"action","action":{"type":"sequence","sequence":[1004,1016],"confidence":0.98},"reasoning":"æ˜ç¢ºãªé€£ç¶šå‹•ä½œã®æŒ‡ç¤º"}

å…¥åŠ›: "å¯æ„›ã„ã­"
å‡ºåŠ›: {"response":"ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼","intent":"action","action":{"type":"single","code":1036,"confidence":0.85},"reasoning":"è¤’ã‚ã‚‰ã‚ŒãŸã®ã§ãƒãƒ¼ãƒˆã§å¿œãˆã‚‹ã€‚å¯¾è©±+å‹•ä½œã®è¤‡åˆæ„å›³"}

å…¥åŠ›: "ç–²ã‚ŒãŸãªã‚"
å‡ºåŠ›: {"response":"ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä¼‘ã¿ã¾ã™ã­","intent":"action","action":{"type":"single","code":1009,"confidence":0.75},"reasoning":"ç–²åŠ´ã®æš—ç¤ºçš„ãªè¡¨ç¾ã‚’ã€Œåº§ã‚‹ã€å‹•ä½œã«è§£é‡ˆ"}

å…¥åŠ›: "ã‚ãªãŸã¯èª°ï¼Ÿ"
å‡ºåŠ›: {"response":"ç§ã¯Claudiaã§ã™ã€‚Unitree Go2ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™","intent":"question","reasoning":"ç´”ç²‹ãªè³ªå•ã€å‹•ä½œä¸è¦"}
"""

PARAMETER num_predict 200
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER stop <|im_end|>
EOF

# 3. åˆ›å»ºæ¨¡å‹
ollama create claudia-intelligent:14b-v2.0 -f models/ClaudiaIntelligent_v2.0.modelfile
```

**ä¼˜åŠ¿**ï¼š
- âœ… **çœŸæ­£çš„è¯­ä¹‰ç†è§£**ï¼šç†è§£éšå–»ï¼ˆ"ç–²ã‚ŒãŸ"â†’åº§ã‚‹ï¼‰
- âœ… **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šå¯ä»¥è®°ä½å¯¹è¯å†å²
- âœ… **è‡ªä¿¡åº¦åé¦ˆ**ï¼šä¸ç¡®å®šæ—¶ä¸»åŠ¨è¯¢é—®
- âœ… **é›¶å…³é”®å­—**ï¼šå®Œå…¨é ç†è§£ï¼Œä¸é åŒ¹é…

**åŠ£åŠ¿**ï¼š
- âš ï¸ å»¶è¿Ÿå¢åŠ ï¼š~3-5ç§’ï¼ˆä½†æ™ºèƒ½æ°´å¹³è´¨å˜ï¼‰
- âš ï¸ GPUæ»¡è½½ï¼š8GBå…¨ç”¨ï¼Œæ— æ³•åŒæ—¶è·‘å…¶ä»–æ¨¡å‹

---

### 1.2 LLMè¾“å‡ºæ ¼å¼ä¼˜åŒ–ï¼ˆStructured Outputï¼‰

**å½“å‰é—®é¢˜**ï¼š
```python
# å½“å‰è§£ææ–¹å¼ï¼ˆå®¹æ˜“å¤±è´¥ï¼‰
response_text = "ä¸€äº›å‰ç¼€æ–‡å­— {\"r\":\"åº§ã‚Šã¾ã™\",\"a\":1009} ä¸€äº›åç¼€"
json_str = response_text[response_text.find("{"):response_text.rfind("}")+1]
result = json.loads(json_str)  # å¯èƒ½å¤±è´¥
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šä½¿ç”¨Ollamaçš„JSONæ¨¡å¼

```python
# production_brain.py ä¸­ä¿®æ”¹ _call_ollama_v2

async def _call_ollama_v2(self, model: str, command: str, timeout: int = 10) -> Optional[Dict]:
    """è°ƒç”¨Ollamaï¼ˆç»“æ„åŒ–è¾“å‡ºæ¨¡å¼ï¼‰"""
    try:
        import ollama

        # å®šä¹‰JSON Schemaï¼ˆå¼ºåˆ¶LLMè¾“å‡ºè§„èŒƒJSONï¼‰
        json_schema = {
            "type": "object",
            "properties": {
                "response": {"type": "string"},
                "intent": {"type": "string", "enum": ["action", "dialog", "question"]},
                "action": {
                    "type": "object",
                    "properties": {
                        "type": {"type": "string", "enum": ["single", "sequence"]},
                        "code": {"type": "integer"},
                        "sequence": {"type": "array", "items": {"type": "integer"}},
                        "confidence": {"type": "number", "minimum": 0, "maximum": 1}
                    }
                },
                "reasoning": {"type": "string"}
            },
            "required": ["response", "intent"]
        }

        # ä½¿ç”¨Ollamaçš„formatå‚æ•°å¼ºåˆ¶JSONè¾“å‡º
        loop = asyncio.get_event_loop()

        def _sync_call():
            return ollama.generate(
                model=model,
                prompt=command,
                format=json_schema,  # âœ… å…³é”®ï¼šå¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º
                options={
                    'temperature': 0.3,
                    'num_predict': 200,
                }
            )

        result = await asyncio.wait_for(
            loop.run_in_executor(None, _sync_call),
            timeout=timeout
        )

        # ç›´æ¥è§£æï¼Œä¸éœ€è¦æŸ¥æ‰¾JSONä½ç½®
        output = json.loads(result['response'])

        # è½¬æ¢ä¸ºBrainOutput
        return self._convert_to_brain_output(output)

    except Exception as e:
        self.logger.error(f"ç»“æ„åŒ–LLMè°ƒç”¨å¤±è´¥: {e}")
        return None

def _convert_to_brain_output(self, llm_output: Dict) -> BrainOutput:
    """è½¬æ¢LLMè¾“å‡ºä¸ºBrainOutput"""
    action = llm_output.get('action', {})

    # å¤„ç†ä¸åŒintent
    if llm_output['intent'] == 'question':
        # çº¯é—®ç­”ï¼Œæ— åŠ¨ä½œ
        return BrainOutput(
            response=llm_output['response'],
            api_code=None,
            confidence=1.0,
            reasoning=llm_output.get('reasoning', 'question_intent')
        )

    elif llm_output['intent'] == 'action':
        # æ£€æŸ¥confidence
        confidence = action.get('confidence', 1.0)

        if confidence < 0.7:
            # ä¸ç¡®å®šï¼Œè¿”å›ç¡®è®¤è¯·æ±‚
            return BrainOutput(
                response=f"{llm_output['response']}ï¼ˆç¢ºèª: ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿï¼‰",
                api_code=None,
                confidence=confidence,
                reasoning=f"low_confidence: {llm_output.get('reasoning')}"
            )

        # é«˜confidenceï¼Œæ‰§è¡ŒåŠ¨ä½œ
        if action['type'] == 'single':
            return BrainOutput(
                response=llm_output['response'],
                api_code=action['code'],
                confidence=confidence,
                reasoning=llm_output.get('reasoning', '')
            )
        else:  # sequence
            return BrainOutput(
                response=llm_output['response'],
                sequence=action['sequence'],
                confidence=confidence,
                reasoning=llm_output.get('reasoning', '')
            )

    else:  # dialog
        return BrainOutput(
            response=llm_output['response'],
            api_code=None,
            confidence=1.0,
            reasoning='dialog_intent'
        )
```

**ä¼˜åŠ¿**ï¼š
- âœ… **100%è§£ææˆåŠŸç‡**ï¼šJSON Schemaå¼ºåˆ¶æ­£ç¡®æ ¼å¼
- âœ… **ç±»å‹å®‰å…¨**ï¼šè‡ªåŠ¨éªŒè¯å­—æ®µç±»å‹
- âœ… **å‡å°‘å¹»è§‰**ï¼šLLMæ›´éš¾äº§ç”Ÿæ— æ•ˆè¾“å‡º
- âœ… **æ€§èƒ½æå‡**ï¼šä¸éœ€è¦å­—ç¬¦ä¸²æŸ¥æ‰¾å’Œæ¸…æ´—

---

## æ–¹æ¡ˆ2ï¼šäº‘ç«¯æ··åˆæ¶æ„ï¼ˆæœ€æ™ºèƒ½ï¼‰

### 2.1 æœ¬åœ°7B + äº‘ç«¯Claude 3.5 Sonnet

**æ¶æ„è®¾è®¡**ï¼š
```python
class IntelligentHybridBrain:
    """
    ä¸‰å±‚æ™ºèƒ½æ¶æ„:
    Layer 1: è§„åˆ™å±‚ï¼ˆä»…ä¿ç•™å®‰å…¨å…³é”®æŒ‡ä»¤ï¼šç´§æ€¥åœæ­¢ï¼‰
    Layer 2: æœ¬åœ°7B LLMï¼ˆå¤„ç†80%å¸¸è§æƒ…å†µï¼Œ<3ç§’ï¼‰
    Layer 3: äº‘ç«¯Claudeï¼ˆå¤„ç†å¤æ‚/æ¨¡ç³Šæƒ…å†µï¼Œ<10ç§’ä½†æå‡†ç¡®ï¼‰
    """

    def __init__(self):
        self.local_llm = ProductionBrain(model="qwen2.5:7b-instruct")
        self.cloud_api = AnthropicClient(model="claude-3-5-sonnet-20241022")

        # ç»Ÿè®¡æŒ‡æ ‡
        self.stats = {
            'local_success': 0,
            'local_low_confidence': 0,
            'cloud_fallback': 0,
            'total_cost': 0.0
        }

    async def process_command(self, command: str, context: List[str] = None) -> BrainOutput:
        """æ™ºèƒ½å¤„ç†æµç¨‹"""

        # Layer 1: ç´§æ€¥æŒ‡ä»¤ï¼ˆç»•è¿‡LLMï¼‰
        if command in ['ç·Šæ€¥åœæ­¢', 'EMERGENCY STOP']:
            return BrainOutput(response="ç·Šæ€¥åœæ­¢", api_code=1003)

        # Layer 2: æœ¬åœ°7Bå°è¯•
        self.logger.info(f"ğŸ§  æœ¬åœ°7Bå¤„ç†: {command}")
        local_result = await self.local_llm.process_with_structured_output(
            command=command,
            context=context,  # ä¼ å…¥å¯¹è¯å†å²
            timeout=5
        )

        # æ£€æŸ¥confidence
        if local_result.confidence >= 0.8:
            self.stats['local_success'] += 1
            self.logger.info(f"âœ… æœ¬åœ°æˆåŠŸ (confidence={local_result.confidence:.2f})")
            return local_result

        # Layer 3: äº‘ç«¯Claude fallback
        self.stats['local_low_confidence'] += 1
        self.logger.warning(
            f"âš ï¸ æœ¬åœ°ç½®ä¿¡åº¦ä½ ({local_result.confidence:.2f}), "
            f"ä½¿ç”¨Claude API: {local_result.reasoning}"
        )

        cloud_result = await self._call_claude_api(command, context, local_result)
        self.stats['cloud_fallback'] += 1
        self.stats['total_cost'] += 0.003  # ~$0.003/è¯·æ±‚

        # ç¼“å­˜Claudeç»“æœä¾›æœ¬åœ°å­¦ä¹ 
        await self._cache_for_learning(command, cloud_result)

        return cloud_result

    async def _call_claude_api(
        self,
        command: str,
        context: List[str],
        local_attempt: BrainOutput
    ) -> BrainOutput:
        """è°ƒç”¨Claude APIï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""

        # æ„å»ºä¸°å¯Œçš„prompt
        conversation_history = "\n".join([
            f"User: {ctx}" for ctx in (context or [])
        ])

        prompt = f"""ã‚ãªãŸã¯å››è¶³ãƒ­ãƒœãƒƒãƒˆçŠ¬Claudiaã®é«˜åº¦ãªçŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

## å¯¾è©±å±¥æ­´
{conversation_history}

## ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
{command}

## ãƒ­ãƒ¼ã‚«ãƒ«LLMã®è©¦è¡Œçµæœï¼ˆå‚è€ƒï¼‰
- è§£é‡ˆ: {local_attempt.reasoning}
- ä¿¡é ¼åº¦: {local_attempt.confidence}
- ææ¡ˆå‹•ä½œ: {local_attempt.api_code or local_attempt.sequence}

## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çœŸã®æ„å›³ã‚’æ·±ãç†è§£ã—ã¦ãã ã•ã„
2. æ›–æ˜§ãªè¡¨ç¾ã‚„æ¯”å–©ã‚‚è§£é‡ˆã—ã¦ãã ã•ã„
3. å¯¾è©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸæ–‡è„ˆç†è§£ã‚’ã—ã¦ãã ã•ã„
4. ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„:

```json
{{
  "response": "æ—¥æœ¬èªã®è‡ªç„¶ãªè¿”äº‹",
  "intent": "action|dialog|question",
  "action": {{
    "type": "single|sequence",
    "code": 1009,
    "sequence": [1004, 1016],
    "confidence": 0.99
  }},
  "reasoning": "è©³ç´°ãªæ¨è«–éç¨‹"
}}
```

åˆ©ç”¨å¯èƒ½ãªå‹•ä½œ: 1004(ç«‹ã¤), 1009(åº§ã‚‹), 1005(ä¼ã›), 1003(åœæ­¢),
1016(æŒ¨æ‹¶), 1017(ã‚¹ãƒˆãƒ¬ãƒƒãƒ), 1036(ãƒãƒ¼ãƒˆ), 1023(ãƒ€ãƒ³ã‚¹),
1030(å‰è»¢), 1031(ã‚¸ãƒ£ãƒ³ãƒ—), 1032(é£›ã³ã‹ã‹ã‚‹)
"""

        try:
            response = await self.cloud_api.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # è§£æClaudeè¿”å›çš„JSON
            content = response.content[0].text

            # Claudeé€šå¸¸è¿”å›markdownåŒ…è£¹çš„JSON
            if "```json" in content:
                json_str = content.split("```json")[1].split("```")[0].strip()
            else:
                json_str = content.strip()

            result = json.loads(json_str)

            return self._convert_to_brain_output(result)

        except Exception as e:
            self.logger.error(f"âŒ Claude APIè°ƒç”¨å¤±è´¥: {e}")
            # Fallbackåˆ°æœ¬åœ°ç»“æœï¼ˆå³ä½¿confidenceä½ï¼‰
            return local_attempt

    async def _cache_for_learning(self, command: str, result: BrainOutput):
        """ç¼“å­˜Claudeç»“æœç”¨äºåç»­Fine-tuning"""
        training_sample = {
            "input": command,
            "output": {
                "response": result.response,
                "api_code": result.api_code,
                "sequence": result.sequence,
                "reasoning": result.reasoning
            },
            "source": "claude",
            "timestamp": datetime.now().isoformat()
        }

        # ä¿å­˜åˆ°è®­ç»ƒæ•°æ®é›†
        with open('logs/training/claude_fallback.jsonl', 'a') as f:
            f.write(json.dumps(training_sample, ensure_ascii=False) + '\n')

        self.logger.info(f"ğŸ’¾ å·²ç¼“å­˜è®­ç»ƒæ ·æœ¬: {command}")
```

**æˆæœ¬åˆ†æ**ï¼š
```python
# å‡è®¾æ¯å¤©100æ¡å‘½ä»¤
# æœ¬åœ°å¤„ç†80% â†’ å…è´¹
# Claudeå¤„ç†20% â†’ 20 * $0.003 = $0.06/å¤©
# æœˆæˆæœ¬: $0.06 * 30 = $1.80/æœˆ

# âœ… å®Œå…¨å¯æ¥å—ï¼ˆç›¸æ¯”æœºå™¨äººç¡¬ä»¶æˆæœ¬å¾®ä¸è¶³é“ï¼‰
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| åœºæ™¯ | æœ¬åœ°7B | äº‘ç«¯Claude |
|------|--------|-----------|
| "åº§ã£ã¦" | âœ… 2s, 95% | â­ 8s, 99.9% |
| "ç–²ã‚ŒãŸ" | âš ï¸ 3s, 70% | â­ 8s, 99% |
| "ç«‹ã£ã¦ãã—ã¦..." | âš ï¸ 3s, 65% | â­ 8s, 99.9% |
| "å¯æ„›ã„ã­" | âš ï¸ 2s, 75% | â­ 8s, 99% |
| "ã‚ãªãŸã¯èª°" | âœ… 1s, 99% | â­ 8s, 99.9% |

**ç”¨æˆ·ä½“éªŒ**ï¼š
- 80%æƒ…å†µï¼š2-3ç§’å“åº”ï¼ˆæœ¬åœ°ï¼‰
- 20%å¤æ‚æƒ…å†µï¼š8-10ç§’å“åº”ï¼ˆäº‘ç«¯ï¼‰- å¯æ¥å—ï¼ˆç”¨æˆ·çŸ¥é“è¿™æ˜¯å¤æ‚ç†è§£ï¼‰
- **å¹³å‡**ï¼š3.2ç§’ï¼ˆ0.8*2.5 + 0.2*9ï¼‰

---

### 2.2 å¯¹è¯å†å²ç®¡ç†ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰

```python
class ConversationManager:
    """ç®¡ç†å¯¹è¯å†å²ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥"""

    def __init__(self, max_history: int = 10):
        self.history: List[Dict] = []
        self.max_history = max_history

    def add_interaction(self, user_input: str, robot_response: str, action: Optional[int]):
        """è®°å½•äº¤äº’"""
        self.history.append({
            'user': user_input,
            'robot': robot_response,
            'action': action,
            'timestamp': time.time()
        })

        # ä¿æŒå†å²é•¿åº¦
        if len(self.history) > self.max_history:
            self.history.pop(0)

    def get_context_for_llm(self) -> List[str]:
        """è·å–LLMå¯ç”¨çš„ä¸Šä¸‹æ–‡"""
        return [
            f"{h['user']} â†’ {h['robot']}"
            for h in self.history[-5:]  # æœ€è¿‘5è½®
        ]

    def resolve_reference(self, command: str) -> str:
        """è§£ææŒ‡ä»£å…³ç³»"""
        # ä¾‹: "ã‚‚ã†ä¸€å›" â†’ å‚è€ƒä¸Šæ¬¡åŠ¨ä½œ
        if "ã‚‚ã†ä¸€å›" in command or "ã‚‚ã†ä¸€åº¦" in command:
            if self.history and self.history[-1]['action']:
                last_action = self.history[-1]['action']
                return f"{command} (å‚è€ƒ: å‰å›ã®å‹•ä½œã¯{last_action})"

        return command

# åœ¨ProductionBrainä¸­é›†æˆ
class ProductionBrain:
    def __init__(self):
        # ...
        self.conversation = ConversationManager()

    async def process_command(self, command: str) -> BrainOutput:
        # è§£ææŒ‡ä»£
        resolved_command = self.conversation.resolve_reference(command)

        # è·å–ä¸Šä¸‹æ–‡
        context = self.conversation.get_context_for_llm()

        # è°ƒç”¨LLMï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
        result = await self.hybrid_brain.process_command(
            resolved_command,
            context=context
        )

        # è®°å½•äº¤äº’
        self.conversation.add_interaction(
            user_input=command,
            robot_response=result.response,
            action=result.api_code
        )

        return result
```

**ç¤ºä¾‹æ•ˆæœ**ï¼š
```
User: åº§ã£ã¦
Robot: åº§ã‚Šã¾ã™ (action: 1009)

User: ãã—ã¦æŒ¨æ‹¶ã—ã¦  # â† çœç•¥äº†ä¸»è¯­
Robot: (ä¸Šä¸‹æ–‡ç†è§£ï¼šå·²ç»åä¸‹) æŒ¨æ‹¶ã—ã¾ã™ (action: 1016)

User: ã‚‚ã†ä¸€å›  # â† æŒ‡ä»£ä¸Šæ¬¡åŠ¨ä½œ
Robot: (ä¸Šä¸‹æ–‡ç†è§£ï¼šé‡å¤æŒ¨æ‹¶) ã‚‚ã†ä¸€åº¦æŒ¨æ‹¶ã—ã¾ã™ (action: 1016)
```

---

## æ–¹æ¡ˆ3ï¼šFine-tuningä¸“ç”¨æ¨¡å‹ï¼ˆé•¿æœŸæœ€ä¼˜ï¼‰

### 3.1 æ•°æ®æ”¶é›†ç­–ç•¥

```bash
# å·²æœ‰æ•°æ®æº
logs/audit/*.jsonl  # å†å²äº¤äº’æ—¥å¿—
logs/training/claude_fallback.jsonl  # Claudeé«˜è´¨é‡æ ‡æ³¨

# éœ€è¦è¡¥å……çš„æ•°æ®
1. è¾¹ç¼˜æ¡ˆä¾‹ï¼ˆæ¨¡ç³Šè¡¨è¾¾ï¼‰
2. é”™è¯¯çº æ­£ï¼ˆç”¨æˆ·åé¦ˆ"ä¸å¯¹ï¼Œæˆ‘æ˜¯è¯´..."ï¼‰
3. å¤šè½®å¯¹è¯æ ·æœ¬
```

### 3.2 Fine-tuningæµç¨‹

```python
# scripts/llm/finetune_qwen.py

import json
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import Dataset

# 1. å‡†å¤‡è®­ç»ƒæ•°æ®
def prepare_training_data():
    """ä»å®¡è®¡æ—¥å¿—æå–è®­ç»ƒæ ·æœ¬"""
    samples = []

    # è¯»å–å®¡è®¡æ—¥å¿—
    for log_file in glob('logs/audit/*.jsonl'):
        with open(log_file) as f:
            for line in f:
                entry = json.loads(line)

                # åªç”¨æˆåŠŸçš„é«˜confidenceæ ·æœ¬
                if entry['success'] and entry.get('confidence', 0) > 0.8:
                    samples.append({
                        'input': entry['input_command'],
                        'output': {
                            'response': entry['llm_output'].get('response'),
                            'api_code': entry['api_code'],
                            'sequence': entry['sequence']
                        }
                    })

    # è¯»å–Claudeæ ‡æ³¨
    with open('logs/training/claude_fallback.jsonl') as f:
        for line in f:
            sample = json.loads(line)
            samples.append(sample)

    return samples

# 2. Fine-tune
def finetune_model():
    base_model = "Qwen/Qwen2.5-7B-Instruct"

    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=True,  # Jetsonå†…å­˜æœ‰é™
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model)

    # å‡†å¤‡æ•°æ®
    samples = prepare_training_data()
    dataset = Dataset.from_list(samples)

    # LoRAé…ç½®ï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=16,  # LoRA rank
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)

    # è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        output_dir="./models/claudia-go2-7b-finetuned",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        num_train_epochs=3,
        learning_rate=2e-4,
        logging_steps=10,
        save_steps=100,
    )

    # è®­ç»ƒ
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
    )

    trainer.train()

    # ä¿å­˜
    model.save_pretrained("./models/claudia-go2-7b-finetuned")

# 3. éƒ¨ç½²åˆ°Ollama
# bash
ollama create claudia-finetuned:7b-v1.0 \
    -f models/claudia-go2-7b-finetuned
```

**é¢„æœŸæ•ˆæœ**ï¼š
- âœ… 7Bå‡†ç¡®ç‡ï¼š70% â†’ 90%+
- âœ… ç†è§£æ—¥è¯­ç»†å¾®å·®åˆ«
- âœ… å‡å°‘Claude fallbackï¼š20% â†’ 5%

---

## å®æ–½è·¯çº¿å›¾

### Phase 1: ç«‹å³æ‰§è¡Œï¼ˆä»Šå¤©ï¼‰
1. âœ… ~~åˆ›å»ºv11.3çº¯æ—¥è¯­Modelfile~~ ï¼ˆå·²å®Œæˆï¼‰
2. â³ **éƒ¨ç½²Qwen2.5-7B + ç»“æ„åŒ–è¾“å‡º**ï¼ˆ2å°æ—¶ï¼‰
3. â³ å®ç°ConversationManagerï¼ˆ1å°æ—¶ï¼‰
4. â³ A/Bæµ‹è¯•7B vs 3Bï¼ˆ1å°æ—¶ï¼‰

### Phase 2: æœ¬å‘¨å®Œæˆ
1. â³ é›†æˆClaude APIï¼ˆæ··åˆæ¶æ„ï¼‰ï¼ˆ3å°æ—¶ï¼‰
2. â³ æ”¶é›†è®­ç»ƒæ•°æ®ï¼ˆä»å®¡è®¡æ—¥å¿—ï¼‰ï¼ˆ2å°æ—¶ï¼‰
3. â³ ä¼˜åŒ–Ollamaé…ç½®ï¼ˆ1å°æ—¶ï¼‰
4. â³ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œç›‘æ§ï¼ˆ2å°æ—¶ï¼‰

### Phase 3: ä¸‹å‘¨å®Œæˆ
1. â³ Fine-tuning Qwen 7Bï¼ˆ1å¤©ï¼‰
2. â³ éƒ¨ç½²Fine-tunedæ¨¡å‹ï¼ˆ2å°æ—¶ï¼‰
3. â³ æ€§èƒ½å¯¹æ¯”å’Œä¼˜åŒ–ï¼ˆ1å¤©ï¼‰

---

## æ€§èƒ½é¢„æœŸå¯¹æ¯”

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | å¹³å‡å»¶è¿Ÿ | æ™ºèƒ½æ°´å¹³ | æˆæœ¬/æœˆ |
|------|--------|----------|----------|---------|
| **å½“å‰(3B+çƒ­è·¯å¾„)** | 65% | 500ms* | â­â­ | $0 |
| **7B+ç»“æ„åŒ–è¾“å‡º** | 85% | 2.5s | â­â­â­â­ | $0 |
| **7B+Claudeæ··åˆ** | 98% | 3.2s | â­â­â­â­â­ | $2 |
| **7B Fine-tuned** | 95% | 2.0s | â­â­â­â­â­ | $0 |

*ä¸»è¦é çƒ­è·¯å¾„ï¼ŒLLMéƒ¨åˆ†ä»3ç§’

---

## æ¨èæ–¹æ¡ˆ

### çŸ­æœŸï¼ˆæœ¬å‘¨ï¼‰ï¼š7B + ç»“æ„åŒ–è¾“å‡º + å¯¹è¯ç®¡ç†
- âœ… é›¶æˆæœ¬
- âœ… å‡†ç¡®ç‡85%ï¼ˆvså½“å‰65%ï¼‰
- âœ… çœŸæ­£æ™ºèƒ½ç†è§£
- âœ… 2-3ç§’å¯æ¥å—å»¶è¿Ÿ

### ä¸­æœŸï¼ˆä¸‹å‘¨ï¼‰ï¼šæ·»åŠ Claudeæ··åˆ
- âœ… å¤„ç†è¾¹ç¼˜æ¡ˆä¾‹
- âœ… å‡†ç¡®ç‡98%
- âœ… æ¯æœˆ$2æˆæœ¬å¯å¿½ç•¥
- âœ… è‡ªåŠ¨æ”¶é›†Fine-tuningæ•°æ®

### é•¿æœŸï¼ˆ2å‘¨åï¼‰ï¼šFine-tuned 7B
- âœ… ä¸“ç”¨æ¨¡å‹ï¼Œæœ€ä¼˜æ€§èƒ½
- âœ… 95%å‡†ç¡®ç‡ï¼Œé›¶æˆæœ¬
- âœ… 2ç§’ä»¥å†…å“åº”

---

**ä½œè€…**: Claude Code
**æœ€åæ›´æ–°**: 2025-11-14 19:00 UTC
